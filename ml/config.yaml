# SLM Training Configuration

# Model Settings
model:
  base_model: "E:\\Git\\Test\\ml\\distilgpt2_model"
  model_save_path: "./trained_model"
  tokenizer_save_path: "./trained_model"
  max_length: 128

# Training Settings
training:
  epochs: 15
  batch_size: 4
  learning_rate: 5.0e-5
  warmup_steps: 100
  weight_decay: 0.01
  gradient_accumulation_steps: 2
  save_steps: 50
  logging_steps: 10
  eval_steps: 50

# LoRA Settings (Parameter-Efficient Fine-Tuning)
lora:
  r: 8
  lora_alpha: 32
  lora_dropout: 0.1
  target_modules: ["c_attn"]

# Data Settings
data:
  train_split: 0.8
  val_split: 0.2
  data_path: "./data.jsonl"
  seed: 42

# Inference Settings
inference:
  temperature: 0.7
  max_new_tokens: 50
  top_p: 0.9
  do_sample: true
